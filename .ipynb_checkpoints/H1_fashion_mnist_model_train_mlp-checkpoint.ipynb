{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Train MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "* Learn how to train an MLP model using PyTorch.\n",
    "* Learn how to export a trained model to ONNX.\n",
    "\n",
    "## References\n",
    "* [Ryzen AI Software Platform](https://ryzenai.docs.amd.com/en/latest/inst.html)\n",
    "\n",
    "* [mnist dataset](https://www.tensorflow.org/datasets/catalog/mnist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set environment\n",
    "\n",
    "**1.1**\n",
    "\n",
    "Follow the instructions on the [Ryzen AI Software Platform](https://ryzenai.docs.amd.com/en/latest/inst.html) website to install the Ryzen AI software platform.\n",
    "\n",
    "**1.2**\n",
    "\n",
    "Activate the created environment and use `pip install <package_name>` to install the necessary packages.\n",
    "\n",
    "**1.3: Import necessary package**\n",
    "\n",
    "Run the following cell to import all the necessary packages to be able to run the inference in the Ryzen AI NPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Dataset\n",
    "We will use the MNIST dataset for training and testing our MLP model. The dataset consists of handwritten digits and is available through the torchvision package.\n",
    "\n",
    "Considering torchvision package can download the MNIST dataset, we use the torchvision API to download.\n",
    "\n",
    "### Downloading the MNIST Dataset\n",
    "\n",
    "* 'root' represents the path of the dataset.\n",
    "* 'train' indicates whether it is the training set.\n",
    "* 'transform = torchvision.transforms.ToTensor()' convert data in PIL Image format to torch.Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 26421880/26421880 [00:12<00:00, 2095784.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 29515/29515 [00:00<00:00, 190216.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 4422102/4422102 [00:02<00:00, 2206220.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 5148/5148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train_data = datasets.MNIST(root=\"./dataset/\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "# test_data  = datasets.MNIST(root=\"./dataset/\", train=False,download=True, transform=torchvision.transforms.ToTensor())\n",
    "train_data = torchvision.datasets.FashionMNIST(\"./data\", download=True, transform=\n",
    "                                                transforms.Compose([transforms.ToTensor()]))\n",
    "test_data = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=\n",
    "                                               transforms.Compose([transforms.ToTensor()]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data length:  60000\n",
      "test data length:  10000\n",
      "train data shape:  2\n",
      "train data image:  torch.Size([1, 28, 28])\n",
      "train data label:  9\n"
     ]
    }
   ],
   "source": [
    "print(\"train data length: \", len(train_data))\n",
    "print(\"test data length: \", len(test_data))\n",
    "\n",
    "print(\"train data shape: \", len(train_data[0]))\n",
    "print(\"train data image: \", train_data[0][0].shape)\n",
    "print(\"train data label: \", train_data[0][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size\n",
    "Batch Size is one of the most important hyperparameters (which is set mannually rather than learned  by the model). \n",
    "\n",
    "Batch Size represents the number of samples in one forward propagation and backward pass. The batch size impact the accuracy and efficiency of the training processing. Small batch size can lead to slow training but relatively high accuray, while a large batch size will result in the opposite. \n",
    "\n",
    "In practice, it is common to try different batch sizes to find the optimal setting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoader\n",
    "DataLoader is a very important interface for PyTorch. Its main function is to encapsulate a custom Dataset into a batch size Tensor based on the batch size, whether to shuffle, etc., for subsequent training.\n",
    "\n",
    "Specifically, DataLoader can do the following:\n",
    "\n",
    "* Batch processing: You can specify the amount of data used for training in each batch (batch size).\n",
    "* Shuffle the data: When training a model, we usually want each epoch to process the samples in the data set in a random order. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True)\n",
    "test_loader  = DataLoader(dataset=test_data, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Buiding a MLP model\n",
    "\n",
    "A Multilayer Perceptron (MLP) is a type of feedforward artificial neural network (ANN). It consists of multiple layers of nodes, including an input layer, one or more hidden layers, and an output layer. Each node, except for those in the input layer, is a neuron that uses a nonlinear activation function. MLPs are capable of learning complex patterns by adjusting the weights of the connections through a process called backpropagation.\n",
    "\n",
    "In this implementation, we will create an MLP by inheriting from the torch.nn.Module class provided by PyTorch. This allows us to leverage the rich functionality provided by PyTorch for defining and managing neural networks. We will override the __init__ method to set up the layers of the network and the forward method to specify the data flow during the forward pass.\n",
    "\n",
    "### Implementation\n",
    "1. Inheriting from torch.nn.Module:\n",
    "\n",
    "* By inheriting from *'torch.nn.Module'*, we can create our custom neural network class. This inheritance allows us to define the architecture and forward propagation logic while taking advantage of PyTorch's built-in features for training and optimization.\n",
    "\n",
    "2. Defining the *'__init__'* method:\n",
    "\n",
    "* In the *'__init__'* method, we initialize the layers of the MLP. This involves defining the linear transformations (fully connected layers) and the activation functions. Each linear layer is represented by *'torch.nn.Linear'*, and we use *'torch.nn.ReLU'* as the activation function between the layers.\n",
    "\n",
    "3. Defining the *'forward'* method:\n",
    "\n",
    "* The *'forward'* method defines the forward pass of the network. It outlines how the input data passes through each layer, undergoing transformations and activations in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, num_input, num_hidden) -> None:\n",
    "        super(MLP,self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(num_input, num_hidden)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(num_hidden, num_hidden)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(num_hidden, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the model\n",
    "We can have an intuitive understanding of the training process, which is to get the best parameters for this model on this dataset.\n",
    "\n",
    "So, we have two questions:\n",
    "* How to define what are the best parameters? (loss function)\n",
    "* How to iterate the parameters of model? (optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize model\n",
    "According to the definition of the MLP class, we need the input data shape and hidden layer shape.\n",
    "\n",
    "* The input data shape is the image size (28x28), which the image matrix has been flattened into a one-dimensional vector.\n",
    "* The hidden layer shape will update while training the MLP model, therefore we just set the initial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = 28*28\n",
    "num_hidden = 320\n",
    "model = MLP(num_input, num_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the device\n",
    "We can select different devices to train the model: GPU or CPU.\n",
    "\n",
    "If you install cuda and torch-cuda successfully, you can use cuda to speed ​​up the training process.If you use cuda as device to train, you should be careful about data transfers between different devices (cuda and cpu). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device) # model transfer to device from 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss function\n",
    "\n",
    "The purpose of the loss function is to measure how well the model predicts. In practice, we define a function named loss function to compute the difference between the predicted value and ground truth.\n",
    "\n",
    "Therefore, the smaller the loss, the better the model.\n",
    "\n",
    "We can have a simple but intuitive understanding of the loss function: the loss can be the predicted value subtracted from the true value. But we often define more complex methods to compute loss.\n",
    "\n",
    "In this case, we define a **'CrossEntropyLoss'** method to compute the difference between the predicted value and ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer\n",
    "\n",
    "We solve the second question: 'How to iterate the parameters of the model?'\n",
    "\n",
    "Use an optimizer to adjust model parameters to minimize the loss function.\n",
    "\n",
    "PyTorch can select different optimizers in **'torch.optim'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epochs\n",
    "\n",
    "Epoch refers to the process of iterating the entire dataset.\n",
    "\n",
    "In one Epoch, the model performs forward propagation and backpropagation on the entire dataset to update all parameters.\n",
    "\n",
    "We set epoch equal to 10 in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training process\n",
    "**train the model**\n",
    "* Get data(image and label) and copy to device\n",
    "* Input image to the model and get the output(predicted value)\n",
    "* Compute loss \n",
    "* Update paramters of model using optimizer\n",
    "* Compare the output and label to compute accuracy\n",
    "\n",
    "**evaluate the model**\n",
    "* Get data(image and label) and copy to device\n",
    "* Input image to the model and get the output(predicted value)\n",
    "* Compute loss\n",
    "* Compare the output and label to compute accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]: train loss:0.500, train acc:81.836%, val loss:0.443, val acc:83.708%\n",
      "[2/10]: train loss:0.360, train acc:86.822%, val loss:0.399, val acc:85.529%\n",
      "[3/10]: train loss:0.326, train acc:87.973%, val loss:0.370, val acc:86.684%\n",
      "[4/10]: train loss:0.302, train acc:88.826%, val loss:0.369, val acc:86.485%\n",
      "[5/10]: train loss:0.284, train acc:89.471%, val loss:0.335, val acc:87.958%\n",
      "[6/10]: train loss:0.267, train acc:89.907%, val loss:0.365, val acc:87.012%\n",
      "[7/10]: train loss:0.255, train acc:90.372%, val loss:0.328, val acc:88.714%\n",
      "[8/10]: train loss:0.243, train acc:90.747%, val loss:0.335, val acc:88.346%\n",
      "[9/10]: train loss:0.232, train acc:91.206%, val loss:0.356, val acc:87.560%\n",
      "[10/10]: train loss:0.220, train acc:91.671%, val loss:0.326, val acc:88.595%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model.train()\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        img, labels = data\n",
    "        img, labels = img.to(device), labels.to(device) # copy data to device\n",
    "        img = torch.flatten(img, start_dim=1)#28*28 matrix flat to 1D vector\n",
    "\n",
    "        outputs = model(img)\n",
    "        loss = loss_fn(outputs,labels)#compute loss value\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #sum loss in one epoch\n",
    "        train_loss += loss.item()\n",
    "        #get the predicted value\n",
    "        _, preds=torch.max(outputs.data,1)\n",
    "        #Check if the predicted value is equal to ground truth and compute accuracy\n",
    "        num_correct = torch.sum(preds == labels).item()\n",
    "        train_acc += num_correct / img.shape[0]\n",
    "\n",
    "    #for this epoch, we evaluate the current model using test dataset \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "\n",
    "    for data in test_loader:\n",
    "        inputs, lables = data\n",
    "        inputs, labels = inputs.to(device), lables.to(device) #copy data to device\n",
    "        inputs = torch.flatten(inputs, start_dim=1) \n",
    "\n",
    "        #get the predicted value \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "\n",
    "        #compute loss and accuracy on test dataset\n",
    "        val_loss += loss.item()\n",
    "        _, preds=torch.max(outputs.data,1)\n",
    "        num_correct = torch.sum(preds==labels.data).item()\n",
    "        val_acc += num_correct / inputs.shape[0]\n",
    "\n",
    "    #print every epoch to check\n",
    "    print('[{}/{}]: train loss:{:.3f}, train acc:{:.3f}%, val loss:{:.3f}, val acc:{:.3f}%'.format(epoch + 1, epochs, \n",
    "        train_loss / len(train_loader), 100*train_acc / len(train_loader),\n",
    "        val_loss / len(test_loader),    100*val_acc / len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save the model\n",
    "We have trained MLP model, now we save the model on our computer (model's parameters and structure). \n",
    "\n",
    "Later, we can not train again when we want to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path:str):\n",
    "    if(not os.path.exists(path)):\n",
    "        os.mkdir(path)\n",
    "    torch.save(model, os.path.join(path, \"mlp_trained.pt \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./models\"\n",
    "save_model(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Export ONNX model\n",
    "We got a trained model after last section, and we can export to onnx format which a standard format for describing computational graphs. ONNX is used as a bridge from deep learning framework to inference engine and we usually just use ONNX to represent computational graphs that are easier to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./models\"\n",
    "torch_model = os.path.join(path, \"mlp_trained.pt \")\n",
    "model = torch.load(torch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.onnx.export API\n",
    "The torch.onnx.export API is to input the image the model and traverse the computional graph, therefore can get the computional graph to export onnx fotmat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = os.path.join(path, \"mlp_trained.onnx\")#onnx model path\n",
    "img = torch.randn([1, 28*28]) #input data\n",
    "input_names = [\"input\"]\n",
    "output_names = ['output']\n",
    "dynamic_axes = {'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}#{0: 'batch_size'} represent 0 dim's shape is dynamic and 0 dim'name is batch size\n",
    "tmp_model_path = os.path.join(path, \"mlp_trained.onnx\")\n",
    "torch.onnx.export(\n",
    "        model,\n",
    "        (img),\n",
    "        onnx_model,\n",
    "        export_params=True,\n",
    "        opset_version=13,\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        dynamic_axes=dynamic_axes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual  Computational graph\n",
    "we can use website: https://netron.app/ to view onnx model.\n",
    ">Netron is a viewer for neural network, deep learning and machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"1000\"\n",
       "            src=\"https://netron.app/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x21cbeed9370>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "notebook_url = \"https://netron.app/\"\n",
    "\n",
    "iframe = IFrame(notebook_url, width=800, height=1000)\n",
    "\n",
    "display(iframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

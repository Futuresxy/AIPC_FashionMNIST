{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Quantization Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "* Learn how to quantize a onnx model using *'vai_q_onnx'* \n",
    "* Learn how to perform inference on an AIE using ONNX runtime.\n",
    "\n",
    "Quantization is the process of converting a model's high-bit floating-point data format (usually 32-bit) into a lower-bit fixed-point data format (e.g., 8-bit integers) without significantly affecting the accuracy of the model's inference. This reduces computational complexity and storage space requirements, making the model more efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Packages\n",
    "\n",
    "First, we need to import the necessary packages to run the inference on the Ryzen AI NPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from onnxruntime.quantization.calibrate import CalibrationDataReader\n",
    "\n",
    "import vai_q_onnx\n",
    "from onnxruntime.quantization import CalibrationDataReader, QuantType, QuantFormat\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Quantize the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize Methods\n",
    "\n",
    "Quantization can be categorized into two main types: Post-training Quantization (PTQ) and Quantization-aware Training (QAT). PTQ can be further divided into dynamic and static methods. Dynamic PTQ maps the model's weights directly from FP32 to INT8, which can lead to significant accuracy loss. Static PTQ, on the other hand, requires a small calibration dataset to compute how to map FP32 weights to INT8, resulting in better accuracy.\n",
    "\n",
    "In this example, we will use the PTQ static method to quantize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Calibration Dataset\n",
    "\n",
    "We will randomly select 1000 samples from the MNIST test dataset as the calibration dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torchvision.datasets.FashionMNIST(root=\"./data/\", train=True, download=False,transform=torchvision.transforms.ToTensor())\n",
    "_, calibration_data = torch.utils.data.random_split(test_data, [len(test_data)-1000,1000])\n",
    "calibration_loader = DataLoader(calibration_data, batch_size = 64, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration dataset loader\n",
    "\n",
    "We need to package the dataset into a dataloader object to facilitate subsequent iterations, similar to the training process. We inherit the *'CalibrationDataReader'* class from *'onnxruntime'* and implement the required *'__init__'* and *'get_next'* methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTCalibrationDatasetLoader(CalibrationDataReader):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.iterator = iter(calibration_loader)  \n",
    "\n",
    "    def get_next(self) -> dict:\n",
    "        try:\n",
    "            images, labels = next(self.iterator)\n",
    "            images = torch.flatten(images, start_dim=1)\n",
    "            return {\"input\": images.numpy()}\n",
    "        except Exception :\n",
    "            return None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize the Model Using\n",
    "We use *'vai_q_onnx.quantize_static'* API from Vitis AI. We need to set some parameters, such as input/output model, calibration data set, parameter type (INT8), quantization method, etc.\n",
    "\n",
    "In addition, we can also set more fine-grained quantification conditions to apply to different hardware and improve model inference performance,such as *‘*enable_dpu’*, *'extra_options'*, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vai_q_onnx.quant_utils:The input ONNX model models/mlp_trained.onnx can create InferenceSession successfully\n",
      "INFO:vai_q_onnx.quantize:Removed initializers from input\n",
      "INFO:vai_q_onnx.quantize:Loading model...\n",
      "INFO:vai_q_onnx.quantize:enable_dpu is True, optimize the model for better hardware compatibility.\n",
      "INFO:vai_q_onnx.quantize:Start calibration...\n",
      "INFO:vai_q_onnx.quantize:Start collecting data, runtime depends on your model size and the number of calibration dataset.\n",
      "INFO:vai_q_onnx.calibrate:Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\n",
      "INFO:vai_q_onnx.calibrate:Use all calibration data to calculate min mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI_Q_ONNX_INFO]: Time information:\n",
      "2024-07-24 08:14:25.962812\n",
      "[VAI_Q_ONNX_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- AUP-MINIPC-D6\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.22621\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 25 Model 116 Stepping 1, AuthenticAMD\n",
      "[VAI_Q_ONNX_INFO]: Tools version information:\n",
      "                                        python --- 3.9.2\n",
      "                                          onnx --- 1.15.0\n",
      "                                   onnxruntime --- 1.15.1\n",
      "                                    vai_q_onnx --- 1.16.0+60e82ab\n",
      "[VAI_Q_ONNX_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- models/mlp_trained.onnx\n",
      "                                  model_output --- models/mlp_qdq.U8S8.onnx\n",
      "                       calibration_data_reader --- <__main__.FashionMNISTCalibrationDatasetLoader object at 0x000002A091CD67F0>\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                random_data_reader_input_shape --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                    enable_dpu --- True\n",
      "                                    debug_mode --- False\n",
      "                             fp16_fp32_convert --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing range: 100%|███████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 61.82tensor/s]\n",
      "INFO:vai_q_onnx.qdq_quantizer:Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model              </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Gemm                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 3                        </span>│\n",
       "│ Relu                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                        </span>│\n",
       "├──────────────────────┼──────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> models/mlp_qdq.U8S8.onnx </span>│\n",
       "└──────────────────────┴──────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model             \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Gemm                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m3                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Relu                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼──────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mmodels/mlp_qdq.U8S8.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴──────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated and quantized model saved at: models/mlp_qdq.U8S8.onnx\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"models/mlp_trained.onnx\"\n",
    "quantization_model = \"models/mlp_qdq.U8S8.onnx\"\n",
    "MNIST_cdr = FashionMNISTCalibrationDatasetLoader()\n",
    "\n",
    "vai_q_onnx.quantize_static(\n",
    "    onnx_model_path,\n",
    "    quantization_model,\n",
    "    MNIST_cdr,\n",
    "    quant_format=QuantFormat.QDQ,\n",
    "    calibrate_method=vai_q_onnx.PowerOfTwoMethod.MinMSE,\n",
    "    activation_type=QuantType.QInt8,\n",
    "    weight_type=QuantType.QInt8,\n",
    "    enable_dpu=True,  # determines whether to generate a quantized model that is suitable for the DPU.\n",
    "    extra_options={'ActivationSymmetric': True} # reduce computation\n",
    ")\n",
    "print('Calibrated and quantized model saved at:', quantization_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Quantized Model\n",
    "In the previous step, we have saved the quantized model in the given file path. \n",
    "\n",
    "Here we need to load the beamed model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_path = './models/mlp_qdq.U8S8.onnx'\n",
    "model = onnx.load(quantized_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Device for Inference\n",
    "\n",
    "Depending on the device you want to use for inference, set the appropriate execution provider:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want inference model on cpu, you can set as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = 'cpu'\n",
    "if(ep == 'cpu'):\n",
    "    providers = ['CPUExecutionProvider']\n",
    "    provider_options = [{}]\n",
    "session = onnxruntime.InferenceSession(model.SerializeToString(), providers=providers,\n",
    "    provider_options=provider_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want inference model on AIE, you can set it as follows.\n",
    "You can leave the optional *\"provider_options.cacheDir\"* and *\"provider_options.cacheKey\"* unset. But 'provider_options.config_file' is **required**.\n",
    "\n",
    "**NOTE: 'vaip_config.json' file is from the Execution Provider setup package. And place the file in the same directory as the above provider_options's 'config_file'**\n",
    "\n",
    "We need copy 'vaip_config.json' file in this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = 'ipu'\n",
    "providers = ['VitisAIExecutionProvider']\n",
    "cache_dir = os.path.join(os.getcwd(), \"onnx\")\n",
    "provider_options = [{\n",
    "        'config_file': 'vaip_config.json',\n",
    "        'cacheDir': str(cache_dir),\n",
    "        'cacheKey': 'mlpmodel'\n",
    "    }]\n",
    "session = onnxruntime.InferenceSession(model.SerializeToString(), providers=providers,\n",
    "    provider_options=provider_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "Randomly select images from the MNIST test dataset for prediction. We use 1000 images in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "test_data  = torchvision.datasets.FashionMNIST(root=\"./data/\",  train=False, download=False,transform=torchvision.transforms.ToTensor())#same root dirctionary\n",
    "print(len(test_data))\n",
    "\n",
    "inference_data_len = 1000\n",
    "size = len(test_data)\n",
    "indice = [random.randint(0,size-1) for _ in range(inference_data_len)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We input 1,000 images into the model, then obtain the output, compare whether the output is consistent with the label, and then judge the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc is 90.1%\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in indice:\n",
    "    img, label = test_data[i]\n",
    "    img = torch.flatten(img, start_dim=1)  \n",
    "    output =  session.run(None, {'input':img.numpy()})\n",
    "    output_arr = output[0]\n",
    "    res=np.argmax(output_arr)\n",
    "    if(res == label):\n",
    "        acc+=1\n",
    "\n",
    "print(f\"acc is {100*acc/inference_data_len}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
